{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14485499,"sourceType":"datasetVersion","datasetId":9252108}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U torch torchvision torchaudio transformers datasets accelerate scikit-learn pandas","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom datasets import Dataset, concatenate_datasets\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForSequenceClassification, \n    TrainingArguments, \n    Trainer,\n    DataCollatorWithPadding\n)\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\n\n# C·∫•u h√¨nh thi·∫øt b·ªã\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODEL_NAME = \"vinai/phobert-large\"\nOUTPUT_DIR = \"./phobert-large-hallu-finetuned\"\n\n# ƒê∆∞·ªùng d·∫´n g·ªëc t·ªõi folder ch·ª©a d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω (D·ª±a theo ·∫£nh b·∫°n g·ª≠i)\n# Gi·∫£ s·ª≠ c·∫•u tr√∫c l√†: /kaggle/input/processed-hal/processed_data_3labels/{T√™n_B·ªô}/train.csv\nDATA_ROOT = \"/kaggle/input/processed-hal/processed_data_3labels/\"\n\n# ƒê·ªãnh nghƒ©a Mapping t·ª´ Ch·ªØ sang S·ªë\nlabel2id = {\n    \"Entailment\": 0,\n    \"Intrinsic-Hal\": 1,\n    \"Extrinsic-Hal\": 2\n}\nid2label = {0: \"Entailment\", 1: \"Intrinsic-Hal\", 2: \"Extrinsic-Hal\"}\n\ndef load_and_map_data(dataset_name, split):\n    # T·∫°o ƒë∆∞·ªùng d·∫´n file\n    file_path = os.path.join(DATA_ROOT, dataset_name, f\"{split}.csv\")\n    \n    if not os.path.exists(file_path):\n        print(f\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file: {file_path}\")\n        return None\n    \n    print(f\"üîÑ ƒêang ƒë·ªçc {dataset_name} - {split}...\")\n    df = pd.read_csv(file_path)\n    \n    # L·ªçc b·ªè d√≤ng l·ªói (n·∫øu c√≥)\n    df = df.dropna(subset=['sentence1', 'sentence2', 'label'])\n    \n    # --- MAP NH√ÉN TR·ª∞C TI·∫æP ---\n    # V√¨ file csv ƒë√£ c√≥ nh√£n ƒë√∫ng t√™n, ta ch·ªâ c·∫ßn map sang s·ªë\n    df['labels'] = df['label'].map(label2id)\n    \n    # Ki·ªÉm tra xem c√≥ nh√£n n√†o l·∫° kh√¥ng (b·ªã NaN sau khi map)\n    if df['labels'].isnull().any():\n        print(f\"‚ö†Ô∏è C·∫£nh b√°o: C√≥ {df['labels'].isnull().sum()} d√≤ng nh√£n l·∫°, s·∫Ω b·ªã b·ªè qua.\")\n        df = df.dropna(subset=['labels'])\n        \n    df['labels'] = df['labels'].astype(int)\n    \n    return df[['sentence1', 'sentence2', 'labels']]\n\n# --- LOAD V√Ä G·ªòP 3 B·ªò D·ªÆ LI·ªÜU ---\ndfs_train = []\ndfs_dev = []\n\n# Danh s√°ch t√™n th∆∞ m·ª•c con c·ªßa 3 b·ªô d·ªØ li·ªáu\ndatasets_list = [\"ViMedNLI\", \"ViANLI\", \"ViNLI\"] \n\nfor ds in datasets_list:\n    t = load_and_map_data(ds, \"train\")\n    d = load_and_map_data(ds, \"dev\")\n    if t is not None: dfs_train.append(t)\n    if d is not None: dfs_dev.append(d)\n\nif not dfs_train:\n    raise ValueError(\"‚ùå L·ªñI: Kh√¥ng load ƒë∆∞·ª£c d·ªØ li·ªáu n√†o! Ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n DATA_ROOT.\")\n\n# G·ªôp DataFrame\nfull_train_df = pd.concat(dfs_train, ignore_index=True)\nfull_dev_df = pd.concat(dfs_dev, ignore_index=True)\n\n# Chuy·ªÉn sang HuggingFace Dataset\ntrain_dataset = Dataset.from_pandas(full_train_df)\ndev_dataset = Dataset.from_pandas(full_dev_df)\n\nprint(\"-\" * 40)\nprint(f\"‚úÖ T·ªïng m·∫´u Train: {len(train_dataset)}\")\nprint(f\"‚úÖ T·ªïng m·∫´u Dev: {len(dev_dataset)}\")\nprint(f\"‚úÖ Ph√¢n b·ªë nh√£n Train: {full_train_df['labels'].value_counts().to_dict()}\")\nprint(f\"   (0: Entailment, 1: Intrinsic-Hal, 2: Extrinsic-Hal)\")\nprint(\"-\" * 40)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\ndef preprocess_function(examples):\n    return tokenizer(\n        examples[\"sentence1\"], \n        examples[\"sentence2\"], \n        truncation=True, \n        padding=False, \n        max_length=256 \n    )\n\nprint(\"‚è≥ ƒêang Tokenize...\")\ntokenized_train = train_dataset.map(preprocess_function, batched=True)\ntokenized_dev = dev_dataset.map(preprocess_function, batched=True)\n\n# X√≥a c·ªôt c≈©\ncols_to_remove = ['sentence1', 'sentence2']\nif '__index_level_0__' in tokenized_train.column_names: cols_to_remove.append('__index_level_0__')\ntokenized_train = tokenized_train.remove_columns(cols_to_remove)\ntokenized_dev = tokenized_dev.remove_columns(cols_to_remove)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_NAME,\n    num_labels=3, # 3 L·ªõp\n    id2label=id2label,\n    label2id=label2id\n).to(device)\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    acc = accuracy_score(labels, predictions)\n    f1 = f1_score(labels, predictions, average='macro')\n    return {\"accuracy\": acc, \"f1_macro\": f1}\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=7,              \n    learning_rate=2e-5,              \n    per_device_train_batch_size=8,   \n    per_device_eval_batch_size=16,\n    gradient_accumulation_steps=2,   \n    \n    # --- C·∫§U H√åNH L∆ØU MODEL TI·∫æT KI·ªÜM ---\n    eval_strategy=\"epoch\",           # V·∫´n ƒë√°nh gi√° sau m·ªói epoch\n    save_strategy=\"epoch\",           # L∆∞u sau m·ªói epoch\n    save_total_limit=1,              # <--- QUAN TR·ªåNG: Ch·ªâ gi·ªØ l·∫°i 1 checkpoint t·ªët nh·∫•t, x√≥a c√°c c√°i c≈©\n    load_best_model_at_end=True,     # Load model t·ªët nh·∫•t khi xong\n    # ------------------------------------\n    \n    metric_for_best_model=\"f1_macro\",\n    weight_decay=0.01,\n    report_to=\"none\",\n    fp16=True,                       \n    logging_steps=100\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_dev,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\n# X√≥a b·ªõt cache c≈© n·∫øu c√≥ ƒë·ªÉ gi·∫£i ph√≥ng dung l∆∞·ª£ng ngay l·∫≠p t·ª©c\nimport shutil\nif os.path.exists(OUTPUT_DIR):\n    shutil.rmtree(OUTPUT_DIR)\n    os.makedirs(OUTPUT_DIR)\n\nprint(\"üöÄ B·∫Øt ƒë·∫ßu Training PhoBERT-Large (3-Class)...\")\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# L∆∞u model\ntrainer.save_model(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\nprint(f\"‚úÖ Model ƒë√£ l∆∞u t·∫°i: {OUTPUT_DIR}\")\n\n# ƒê√°nh gi√° chi ti·∫øt\nprint(\"\\n--- Classification Report (Dev Set) ---\")\npreds_output = trainer.predict(tokenized_dev)\ny_preds = np.argmax(preds_output.predictions, axis=1)\ny_true = preds_output.label_ids\n\ntarget_names = [\"Entailment\", \"Intrinsic-Hal\", \"Extrinsic-Hal\"]\nprint(classification_report(y_true, y_preds, target_names=target_names, digits=4))\n\n# L∆∞u k·∫øt qu·∫£ ra CSV\ndf_res = pd.DataFrame({\"True\": y_true, \"Pred\": y_preds})\ndf_res.to_csv(\"phobert_3class_results.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
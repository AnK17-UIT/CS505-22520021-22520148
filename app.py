import gradio as gr
import torch
import os
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# C·∫•u h√¨nh CPU
BASE_MODEL_NAME = "Qwen/Qwen3-4B-Instruct-2507"
ADAPTER_PATH = "./results/Qwen-Final-Unified-NLI" 

print("Kh·ªüi ƒë·ªông tr√™n CPU")

# Load Tokenizer
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

# Load Base Model
try:
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_NAME,
        device_map="cpu", 
        trust_remote_code=True,
        torch_dtype=torch.float32 
    )
except Exception as e:
    print(f"L·ªói load model: {e}")
    exit()

# Load Adapter
if os.path.exists(ADAPTER_PATH):
    print("ƒêang g·∫Øn Adapter v√†o Base Model...")
    model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)
    model.eval()
    print("Load Adapter th√†nh c√¥ng!")
else:
    print(f"Kh√¥ng t√¨m th·∫•y Adapter t·∫°i: {ADAPTER_PATH}")
    exit()

# --- H√ÄM H·∫¨U X·ª¨ L√ù NH√ÉN (M·ªöI TH√äM) ---
def map_label(raw_output):
    """
    Chuy·ªÉn ƒë·ªïi nh√£n NLI sang nh√£n Hallucination Detection
    """
    raw_lower = raw_output.lower().strip()
    
    if "entailment" in raw_lower:
        return "‚úÖ Entailment (Tin c·∫≠y)"
    elif "contradiction" in raw_lower:
        return "‚ùå Intrinsic-Hal (M√¢u thu·∫´n)"
    elif "neutral" in raw_lower:
        return "‚ö†Ô∏è Extrinsic-Hal (B·ªãa ƒë·∫∑t/Kh√¥ng ki·ªÉm ch·ª©ng)"
    else:
        # Tr∆∞·ªùng h·ª£p model tr·∫£ l·ªùi linh tinh ho·∫∑c ƒëang suy nghƒ© (thinking process)
        return f"‚ùì Unknown ({raw_output})"

# Predict
def format_prompt(context, statement, domain):
    if domain == "Y t·∫ø (ViMedNLI)":
        role = "You are a medical AI assistant."
        note = ""
    else:
        role = "You are an AI expert in Vietnamese Natural Language Inference (NLI)."
        note = "Note: The input text covers various domains and may contain complex, tricky phrasing or subtle logical traps. Analyze carefully."

    prompt = f"""{role} Your task is to determine the logical relationship between the Context and the Statement.
{note}
Context: {context}
Statement: {statement}

Based on the context, classify the statement as one of the following:
- entailment
- neutral
- contradiction

Answer:
"""
    return prompt

def predict_comparison(context, statement, domain):
    if not context or not statement:
        return "‚ö†Ô∏è Ch∆∞a nh·∫≠p d·ªØ li·ªáu", "‚ö†Ô∏è Ch∆∞a nh·∫≠p d·ªØ li·ªáu"

    full_prompt = format_prompt(context, statement, domain)
    
    messages = [{"role": "user", "content": full_prompt}]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    
    inputs = tokenizer([text], return_tensors="pt").to("cpu")

    # 1. Model Fine-tuned
    model.enable_adapter_layers()
    with torch.no_grad():
        outputs_ft = model.generate(
            **inputs, 
            max_new_tokens=30, 
            pad_token_id=tokenizer.eos_token_id,
            temperature=0.1, 
            do_sample=False
        )
    raw_ft = tokenizer.decode(outputs_ft[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()
    result_ft = map_label(raw_ft) # <--- √Åp d·ª•ng h√†m map nh√£n

    # 2. Model Base
    with model.disable_adapter():
        with torch.no_grad():
            outputs_base = model.generate(
                **inputs, 
                max_new_tokens=30, 
                pad_token_id=tokenizer.eos_token_id,
                temperature=0.1, 
                do_sample=False
            )
    raw_base = tokenizer.decode(outputs_base[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()
    result_base = map_label(raw_base) # <--- √Åp d·ª•ng h√†m map nh√£n

    return result_ft, result_base

# Giao di·ªán
custom_css = """
.output-box textarea { 
    font-size: 20px !important; 
    font-weight: bold !important; 
}
"""

with gr.Blocks(title="NLI Local Demo (CPU)") as demo:
    gr.Markdown("# üïµÔ∏è H·ªá th·ªëng Ph√°t hi·ªán Hallucination (Local Demo)")
    gr.Markdown("Ch·∫°y tr√™n CPU - So s√°nh gi·ªØa Base Model v√† Fine-tuned Model (QLoRA)")
    
    with gr.Row():
        with gr.Column():
            inp_domain = gr.Dropdown(
                ["Y t·∫ø (ViMedNLI)", "ƒêa lƒ©nh v·ª±c"], 
                value="ƒêa lƒ©nh v·ª±c", label="Domain"
            )
            inp_context = gr.Textbox(lines=5, placeholder="Nh·∫≠p ng·ªØ c·∫£nh (Context)...", label="Context")
            inp_statement = gr.Textbox(lines=2, placeholder="Nh·∫≠p nh·∫≠n ƒë·ªãnh (Statement)...", label="Statement")
            
            with gr.Row():
                btn_run = gr.Button("üöÄ Ch·∫°y D·ª± ƒêo√°n", variant="primary")
                btn_clear = gr.ClearButton([inp_context, inp_statement])

        with gr.Column():
            gr.Markdown("### üìä K·∫øt qu·∫£ Ph√¢n t√≠ch")
            out_ft = gr.Textbox(label="Fine-tuned Model (ƒê·ªÅ xu·∫•t)", elem_classes="output-box")
            out_base = gr.Textbox(label="Base Model (G·ªëc)", elem_classes="output-box")

    btn_run.click(predict_comparison, [inp_context, inp_statement, inp_domain], [out_ft, out_base])

if __name__ == "__main__":
    demo.launch(
        server_name="localhost", 
        server_port=7860, 
        share=False,
        theme=gr.themes.Soft(),
        css=custom_css
    )
import gradio as gr
import torch
import os
import numpy as np
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    AutoModelForSequenceClassification
)
from peft import PeftModel

# Qwen Config
QWEN_BASE_NAME = "Qwen/Qwen3-4B-Instruct-2507"
QWEN_ADAPTER_PATH = "./results/Qwen-Final-Unified-NLI"

# PhoBERT Config
PHOBERT_PATH = "./results/phobert-large-hallu-finetuned"

print("‚è≥ ƒêang kh·ªüi ƒë·ªông h·ªá th·ªëng tr√™n CPU...")
# Load model
print("--- Loading Qwen Tokenizer ---")
qwen_tokenizer = AutoTokenizer.from_pretrained(QWEN_BASE_NAME, trust_remote_code=True)
if qwen_tokenizer.pad_token is None:
    qwen_tokenizer.pad_token = qwen_tokenizer.eos_token
qwen_tokenizer.padding_side = "left"

print("--- Loading Qwen Base Model ---")
try:
    qwen_base = AutoModelForCausalLM.from_pretrained(
        QWEN_BASE_NAME,
        device_map="cpu",
        trust_remote_code=True,
        torch_dtype=torch.float32
    )
except Exception as e:
    print(f"‚ùå L·ªói load Qwen Base: {e}")
    exit()

if os.path.exists(QWEN_ADAPTER_PATH):
    print("--- Loading Qwen Adapter ---")
    qwen_model = PeftModel.from_pretrained(qwen_base, QWEN_ADAPTER_PATH)
    qwen_model.eval()
    print("‚úÖ Qwen Loaded Successfully!")
else:
    print(f"‚ùå Kh√¥ng t√¨m th·∫•y Adapter t·∫°i: {QWEN_ADAPTER_PATH}")
    exit()

print("--- Loading PhoBERT-Large ---")
if os.path.exists(PHOBERT_PATH):
    try:
        phobert_tokenizer = AutoTokenizer.from_pretrained(PHOBERT_PATH)
        phobert_model = AutoModelForSequenceClassification.from_pretrained(PHOBERT_PATH)
        phobert_model.to("cpu")
        phobert_model.eval()
        print("‚úÖ PhoBERT Loaded Successfully!")
    except Exception as e:
        print(f"‚ùå L·ªói load PhoBERT: {e}")
        phobert_model = None
else:
    print(f"‚ö†Ô∏è C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y folder PhoBERT t·∫°i {PHOBERT_PATH}. Ch·∫ø ƒë·ªô PhoBERT s·∫Ω b·ªã t·∫Øt.")
    phobert_model = None


# --- Map nh√£n cho Qwen (Sinh vƒÉn b·∫£n) ---
def map_label_qwen(raw_output):
    raw_lower = raw_output.lower().strip()
    if "entailment" in raw_lower:
        return "‚úÖ Entailment (Tin c·∫≠y)"
    elif "contradiction" in raw_lower:
        return "‚ùå Intrinsic-Hal (M√¢u thu·∫´n)"
    elif "neutral" in raw_lower:
        return "‚ö†Ô∏è Extrinsic-Hal (B·ªãa ƒë·∫∑t)"
    else:
        return f"‚ùì Unknown ({raw_output})"

# --- Map nh√£n cho PhoBERT (Ph√¢n lo·∫°i) ---
# Gi·∫£ ƒë·ªãnh th·ª© t·ª± nh√£n l√∫c train PhoBERT l√†: 0: Entailment, 1: Intrinsic, 2: Extrinsic
# N·∫øu b·∫°n train kh√°c th·ª© t·ª±, h√£y s·ª≠a l·∫°i dict n√†y
phobert_id2label = {
    0: "‚úÖ Entailment (Tin c·∫≠y)",
    1: "‚ùå Intrinsic-Hal (M√¢u thu·∫´n)",
    2: "‚ö†Ô∏è Extrinsic-Hal (B·ªãa ƒë·∫∑t)"
}

def format_prompt_qwen(context, statement, domain):
    if domain == "Y t·∫ø (ViMedNLI)":
        role = "You are a medical AI assistant."
        note = ""
    else:
        role = "You are an AI expert in Vietnamese Natural Language Inference (NLI)."
        note = "Note: The input text covers various domains and may contain complex, tricky phrasing or subtle logical traps. Analyze carefully."

    return f"""{role} Your task is to determine the logical relationship between the Context and the Statement.
{note}
Context: {context}
Statement: {statement}

Based on the context, classify the statement as one of the following:
- entailment
- neutral
- contradiction

Answer:
"""

def predict_all(context, statement, domain):
    if not context or not statement:
        return "‚ö†Ô∏è Tr·ªëng", "‚ö†Ô∏è Tr·ªëng", "‚ö†Ô∏è Tr·ªëng"

    # --- 1. D·ª∞ ƒêO√ÅN V·ªöI QWEN (Fine-tuned & Base) ---
    full_prompt = format_prompt_qwen(context, statement, domain)
    messages = [{"role": "user", "content": full_prompt}]
    text = qwen_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = qwen_tokenizer([text], return_tensors="pt").to("cpu")

    # Qwen Fine-tuned
    qwen_model.enable_adapter_layers()
    with torch.no_grad():
        out_ft = qwen_model.generate(**inputs, max_new_tokens=30, pad_token_id=qwen_tokenizer.eos_token_id, temperature=0.1, do_sample=False)
    res_ft = map_label_qwen(qwen_tokenizer.decode(out_ft[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip())

    # Qwen Base
    with qwen_model.disable_adapter():
        with torch.no_grad():
            out_base = qwen_model.generate(**inputs, max_new_tokens=30, pad_token_id=qwen_tokenizer.eos_token_id, temperature=0.1, do_sample=False)
    res_base = map_label_qwen(qwen_tokenizer.decode(out_base[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip())

    # --- 2. D·ª∞ ƒêO√ÅN V·ªöI PHOBERT ---
    if phobert_model:
        # PhoBERT n·ªëi c√¢u b·∫±ng token ƒë·∫∑c bi·ªát (<s> sentence1 </s> </s> sentence2 </s>)
        # Tokenizer c·ªßa PhoBERT t·ª± x·ª≠ l√Ω vi·ªác n√†y khi truy·ªÅn 2 c√¢u
        phobert_inputs = phobert_tokenizer(
            context, 
            statement, 
            return_tensors="pt", 
            truncation=True, 
            max_length=256,
            padding=True
        ).to("cpu")
        
        with torch.no_grad():
            logits = phobert_model(**phobert_inputs).logits
            probs = torch.nn.functional.softmax(logits, dim=-1)
            pred_idx = torch.argmax(probs, dim=-1).item()
            confidence = probs[0][pred_idx].item()
            
        label_text = phobert_id2label.get(pred_idx, "Unknown")
        res_phobert = f"{label_text}\n(ƒê·ªô tin c·∫≠y: {confidence:.2%})"
    else:
        res_phobert = "‚ö†Ô∏è Model not loaded"

    return res_ft, res_base, res_phobert

custom_css = """
.output-box textarea { 
    font-size: 18px !important; 
    font-weight: bold !important; 
}
"""

with gr.Blocks(title="Hallucination Detection System", theme=gr.themes.Soft(), css=custom_css) as demo:
    gr.Markdown("# üïµÔ∏è H·ªá th·ªëng Ph√°t hi·ªán Hallucination (Multi-Model)")
    gr.Markdown("So s√°nh k·∫øt qu·∫£ gi·ªØa **Qwen-4B (LLM)** v√† **PhoBERT-Large (Encoder)** tr√™n CPU.")
    
    with gr.Row():
        # C·ªôt Input
        with gr.Column(scale=1):
            inp_domain = gr.Dropdown(
                ["Y t·∫ø (ViMedNLI)", "ƒêa lƒ©nh v·ª±c"], 
                value="ƒêa lƒ©nh v·ª±c", label="Lƒ©nh v·ª±c (Domain)"
            )
            inp_context = gr.Textbox(lines=6, placeholder="Nh·∫≠p ng·ªØ c·∫£nh...", label="Context")
            inp_statement = gr.Textbox(lines=3, placeholder="Nh·∫≠p nh·∫≠n ƒë·ªãnh...", label="Statement")
            
            with gr.Row():
                btn_run = gr.Button("üöÄ Ph√¢n t√≠ch", variant="primary")
                gr.ClearButton([inp_context, inp_statement])

        # C·ªôt Output
        with gr.Column(scale=1):
            gr.Markdown("### üèÜ K·∫øt qu·∫£ Ph√¢n t√≠ch")
            
            # Group 1: Qwen
            with gr.Group():
                gr.Markdown("#### ü§ñ Qwen-4B (Fine-tuned w/ QLoRA)")
                out_ft = gr.Textbox(label="K·∫øt qu·∫£", elem_classes="output-box")
            
            # Group 2: PhoBERT
            with gr.Group():
                gr.Markdown("#### ü¶Ö PhoBERT-Large (Fine-tuned)")
                out_phobert = gr.Textbox(label="K·∫øt qu·∫£", elem_classes="output-box")
            
            gr.Markdown("---")
            
            # Group 3: Base Model (Tham chi·∫øu)
            with gr.Group():
                gr.Markdown("#### üë∂ Qwen-4B Base (G·ªëc)")
                out_base = gr.Textbox(label="K·∫øt qu·∫£", elem_classes="output-box")

    btn_run.click(
        predict_all, 
        [inp_context, inp_statement, inp_domain], 
        [out_ft, out_base, out_phobert]
    )

if __name__ == "__main__":
    demo.launch(server_name="localhost", server_port=7860, share=False)

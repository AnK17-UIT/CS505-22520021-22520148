{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316adb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U torch torchvision torchaudio transformers datasets accelerate scikit-learn pandas\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification, \n",
    "    TrainingArguments, Trainer, DataCollatorWithPadding\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "DATASET_NAME = \"ViMedNLI\"\n",
    "MODEL_NAME = \"vinai/phobert-large\"\n",
    "OUTPUT_DIR = f\"./phobert-large-{DATASET_NAME.lower()}\"\n",
    "# S·ª≠a ƒë∆∞·ªùng d·∫´n n√†y n·∫øu c·∫ßn\n",
    "DATA_ROOT = \"/kaggle/input/processed-hal/processed_data_3labels/\" \n",
    "\n",
    "# --- X·ª¨ L√ù D·ªÆ LI·ªÜU ---\n",
    "label2id = {\"Entailment\": 0, \"Intrinsic-Hal\": 1, \"Extrinsic-Hal\": 2}\n",
    "id2label = {0: \"Entailment\", 1: \"Intrinsic-Hal\", 2: \"Extrinsic-Hal\"}\n",
    "\n",
    "def load_data(split):\n",
    "    path = os.path.join(DATA_ROOT, DATASET_NAME, f\"{split}.csv\")\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.dropna(subset=['sentence1', 'sentence2', 'label'])\n",
    "    # Map nh√£n text sang s·ªë\n",
    "    df['labels'] = df['label'].map(label2id)\n",
    "    return Dataset.from_pandas(df)\n",
    "\n",
    "print(f\"‚è≥ ƒêang load d·ªØ li·ªáu {DATASET_NAME}...\")\n",
    "train_dataset = load_data(\"train\")\n",
    "dev_dataset = load_data(\"dev\")\n",
    "print(f\"‚úÖ Train: {len(train_dataset)} | Dev: {len(dev_dataset)}\")\n",
    "\n",
    "# --- TOKENIZER ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "def preprocess(examples):\n",
    "    return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, padding=False, max_length=256)\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess, batched=True)\n",
    "tokenized_dev = dev_dataset.map(preprocess, batched=True)\n",
    "\n",
    "# --- MODEL & METRICS ---\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=3, id2label=id2label, label2id=label2id\n",
    ").to(\"cuda\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1_macro\": f1_score(labels, preds, average='macro')\n",
    "    }\n",
    "\n",
    "# --- TRAINING (FIX L·ªñI TR√ÄN ·ªî C·ª®NG) ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=7,              # Train k·ªπ 7 epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,   # Batch nh·ªè cho model Large\n",
    "    gradient_accumulation_steps=2,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,              # <--- QUAN TR·ªåNG: Ch·ªâ gi·ªØ 1 checkpoint t·ªët nh·∫•t\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args,\n",
    "    train_dataset=tokenized_train, eval_dataset=tokenized_dev,\n",
    "    tokenizer=tokenizer, data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"üöÄ B·∫Øt ƒë·∫ßu Training...\")\n",
    "trainer.train()\n",
    "\n",
    "# --- L∆ØU & ƒê√ÅNH GI√Å ---\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"‚úÖ ƒê√£ l∆∞u model t·∫°i {OUTPUT_DIR}\")\n",
    "\n",
    "preds = np.argmax(trainer.predict(tokenized_dev).predictions, axis=1)\n",
    "print(classification_report(tokenized_dev['labels'], preds, target_names=list(label2id.keys()), digits=4))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

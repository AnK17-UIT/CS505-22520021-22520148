{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T04:05:01.446644Z",
     "iopub.status.busy": "2025-12-15T04:05:01.446433Z",
     "iopub.status.idle": "2025-12-15T04:09:05.001700Z",
     "shell.execute_reply": "2025-12-15T04:09:05.000656Z",
     "shell.execute_reply.started": "2025-12-15T04:05:01.446625Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m899.8/899.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m170.4/170.4 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m517.4/517.4 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
      "pylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
      "cudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.8.0 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.8.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\n",
      "cudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Cell 1: CÃ i Ä‘áº·t Ä‘á»“ng bá»™\n",
    "!pip install -q -U torch torchvision torchaudio transformers peft datasets bitsandbytes trl accelerate pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Inference on GPU \n",
    "Model page: https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507\n",
    "\n",
    "âš ï¸ If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507)\n",
    "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) ğŸ™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T04:09:05.004098Z",
     "iopub.status.busy": "2025-12-15T04:09:05.003826Z",
     "iopub.status.idle": "2025-12-15T04:09:36.142274Z",
     "shell.execute_reply": "2025-12-15T04:09:36.141588Z",
     "shell.execute_reply.started": "2025-12-15T04:09:05.004069Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-15 04:09:15.118667: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765771755.318773      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765771755.375455      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Import Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig, \n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "# Import PEFT & TRL\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "try:\n",
    "    del model\n",
    "    del trainer\n",
    "    del tokenizer\n",
    "except:\n",
    "    pass\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Environment config\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T04:09:36.143537Z",
     "iopub.status.busy": "2025-12-15T04:09:36.143224Z",
     "iopub.status.idle": "2025-12-15T04:09:36.149785Z",
     "shell.execute_reply": "2025-12-15T04:09:36.148879Z",
     "shell.execute_reply.started": "2025-12-15T04:09:36.143510Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Transformers version: 2.9.1+cu128\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "MODEL_NAME = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "NEW_MODEL_NAME = \"Qwen-ViMedNLI-Hallucination-Detection\"\n",
    "\n",
    "# Config data path\n",
    "DATA_PATH = \"/kaggle/input/processed/processed_data_3labels/ViMedNLI/csv_processed/\"\n",
    "TRAIN_FILE = os.path.join(DATA_PATH, \"train.csv\")\n",
    "DEV_FILE = os.path.join(DATA_PATH, \"dev.csv\")\n",
    "TEST_FILE = os.path.join(DATA_PATH, \"test.csv\")\n",
    "\n",
    "# Thiáº¿t láº­p device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Kiá»ƒm tra xem import cÃ³ thÃ nh cÃ´ng khÃ´ng\n",
    "print(\"Transformers version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T04:09:36.151273Z",
     "iopub.status.busy": "2025-12-15T04:09:36.150952Z",
     "iopub.status.idle": "2025-12-15T04:09:37.179331Z",
     "shell.execute_reply": "2025-12-15T04:09:37.178546Z",
     "shell.execute_reply.started": "2025-12-15T04:09:36.151245Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample train data {'messages': [{'content': 'You are a medical AI assistant. Your task is to determine the logical relationship between the following Context and Statement.\\n        \\n        Context: cÃ¡c thá»­ nghiá»‡m Ä‘Ã¡ng chÃº Ã½ vá»›i cr giÃ¡ tiá»n baseline giÃ¡ tiá»n trÃªn má»—i há»“ sÆ¡ cÅ© váº§ lactate giÃ¡ tiá»n\\n        Statement: bá»‡nh nhÃ¢n cÃ³ ná»“ng Ä‘á»™ cr giÃ¡ tiá»n mol l tÄƒng cr\\n\\n        Based on the context, classify the statement as one of the followings:\\n        - Entailment: The statement is definitely true based on the context.\\n        - Neutral: The truth of the statement cannot be determined from the context.\\n        - Contradiction: The statement is definitely false based on the context.\\n\\n        Answer:\\n        ', 'role': 'user'}, {'content': 'entailment', 'role': 'assistant'}], 'label_str': 'entailment'}\n"
     ]
    }
   ],
   "source": [
    "# Load and process function\n",
    "def load_and_process_data(train_path, dev_path, test_path):\n",
    "    # Read CSV\n",
    "    df_train = pd.read_csv(train_path)\n",
    "    df_dev = pd.read_csv(dev_path)\n",
    "    df_test = pd.read_csv(test_path)\n",
    "\n",
    "    label_map = {\n",
    "        0: \"Entailment\",\n",
    "        1: \"Extrinsic-Hal\",\n",
    "        2: \"Intrinsic-Hal\"\n",
    "    }\n",
    "\n",
    "    def format_instruction(row):\n",
    "        prompt = f\"\"\"You are a medical AI assistant. Your task is to determine the logical relationship between the following Context and Statement.\n",
    "        \n",
    "        Context: {row['sentence1']}\n",
    "        Statement: {row['sentence2']}\n",
    "\n",
    "        Based on the context, classify the statement as one of the followings:\n",
    "        - Entailment: The statement is definitely true based on the context.\n",
    "        - Extrinsic-hal: The truth of the statement cannot be determined from the context (it might be true or false, but the context doesnâ€™t say).\n",
    "        - Intrinsic-hal: The statement is definitely false or contradicts the context.\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        return prompt\n",
    "\n",
    "    \n",
    "    def process_df(df):\n",
    "        data = []\n",
    "        for _, row in df.iterrows():\n",
    "            user_content = format_instruction(row)\n",
    "            assistant_content = label_map.get(row['label'], str(row['label']))\n",
    "\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": user_content},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "            ]\n",
    "\n",
    "            data.append({\"messages\": messages, \"label_str\": assistant_content})\n",
    "        return Dataset.from_list(data)\n",
    "\n",
    "    train_dataset = process_df(df_train)\n",
    "    dev_dataset = process_df(df_dev)\n",
    "    test_dataset = process_df(df_test)\n",
    "\n",
    "    return DatasetDict({\"train\": train_dataset, \"dev\": dev_dataset, \"test\": test_dataset})\n",
    "\n",
    "# Load data\n",
    "dataset = load_and_process_data(TRAIN_FILE, DEV_FILE, TEST_FILE)\n",
    "print(\"Sample train data\", dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T04:09:37.181569Z",
     "iopub.status.busy": "2025-12-15T04:09:37.181265Z",
     "iopub.status.idle": "2025-12-15T04:10:22.226311Z",
     "shell.execute_reply": "2025-12-15T04:10:22.225230Z",
     "shell.execute_reply.started": "2025-12-15T04:09:37.181549Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65dfae2356f34df8bd7231cbf03d2802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed24766caa8f4f8aa9ef71883b1e9793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e789d539e841788572980b632713a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a51da610c341498282e48bdcb149f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1833cc58bf444a4c99e8b70957cb0732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b514bc6af3ff4e2299ac477b49cb22d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a5cd4b189aa46a292e79c37d9a6a749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57508ba2ceb64e2bb074e0a383a329ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85cfddd53c1842f8ac301dc0fb5cd2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/99.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b8a6212e104a4caef69e8438df6719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1030602fa0f435dbb0a3fe178f1ffab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820678402a634777bb2c6904240ba18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/238 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Config Quantization (T4 báº¯t buá»™c dÃ¹ng float16)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16, \n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load Model (QUAN TRá»ŒNG: torch_dtype=torch.float16)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16  # <--- Ã‰p kiá»ƒu ngay tá»« khi load\n",
    ")\n",
    "\n",
    "# Táº¯t cache vÃ  chuáº©n bá»‹ model\n",
    "model.config.use_cache = False \n",
    "model.config.pretraining_tp = 1\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# --- 2. Cáº¥u hÃ¬nh LoRA ---\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=16, \n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T04:10:22.228546Z",
     "iopub.status.busy": "2025-12-15T04:10:22.227442Z",
     "iopub.status.idle": "2025-12-15T04:10:34.907180Z",
     "shell.execute_reply": "2025-12-15T04:10:34.905678Z",
     "shell.execute_reply.started": "2025-12-15T04:10:22.228518Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c48686c4527141b0b2ae12f04ba365a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/11232 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588b076fbb734cfea5040593c8fb55e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/11232 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a754c201d94e5bb278b731a827ec01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/1395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6810f58c28741ad80e31b5ba3227a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/1395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 3. Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=100,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,   # T4 dÃ¹ng FP16\n",
    "    bf16=False,  # T4 KHÃ”NG há»— trá»£ BF16 -> Pháº£i lÃ  False\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"none\",\n",
    "    gradient_checkpointing=True \n",
    ")\n",
    "\n",
    "# --- 4. Khá»Ÿi táº¡o Trainer ---\n",
    "# (Giáº£ sá»­ biáº¿n 'dataset' Ä‘Ã£ Ä‘Æ°á»£c táº¡o tá»« cell load data trÆ°á»›c Ä‘Ã³)\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['dev'],\n",
    "    peft_config=peft_config,\n",
    "    args=training_args,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T04:10:34.909202Z",
     "iopub.status.busy": "2025-12-15T04:10:34.908960Z",
     "iopub.status.idle": "2025-12-15T04:10:34.957697Z",
     "shell.execute_reply": "2025-12-15T04:10:34.956607Z",
     "shell.execute_reply.started": "2025-12-15T04:10:34.909182Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Hybrid Cast for T4 compatibility...\n",
      "Upcast trainable param base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.32.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.32.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.32.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.32.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.32.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.32.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.32.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.32.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.32.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.32.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.32.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.32.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.32.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.32.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.33.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.33.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.33.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.33.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.33.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.33.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.33.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.33.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.33.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.33.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.33.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.33.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.33.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.33.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.34.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.34.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.34.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.34.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.34.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.34.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.34.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.34.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.34.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.34.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.34.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.34.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.34.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.34.mlp.down_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.35.self_attn.q_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.35.self_attn.q_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.35.self_attn.k_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.35.self_attn.k_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.35.self_attn.v_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.35.self_attn.v_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.35.self_attn.o_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.35.self_attn.o_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.35.mlp.gate_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.35.mlp.gate_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.35.mlp.up_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.35.mlp.up_proj.lora_B.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.35.mlp.down_proj.lora_A.default.weight to float32\n",
      "Upcast trainable param base_model.model.model.layers.35.mlp.down_proj.lora_B.default.weight to float32\n",
      "Hybrid Cast complete. LoRA is FP32, Base is FP16/INT4.\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying Hybrid Cast for T4 compatibility...\")\n",
    "\n",
    "# Duyá»‡t qua táº¥t cáº£ tham sá»‘\n",
    "for name, param in trainer.model.named_parameters():\n",
    "    # TRÆ¯á»œNG Há»¢P 1: Tham sá»‘ cáº§n train (LoRA) -> Báº¯t buá»™c Float32 Ä‘á»ƒ trÃ¡nh lá»—i 'unscale FP16'\n",
    "    if param.requires_grad:\n",
    "        if param.dtype != torch.float32:\n",
    "            param.data = param.data.to(torch.float32)\n",
    "            print(f\"Upcast trainable param {name} to float32\")\n",
    "            \n",
    "    # TRÆ¯á»œNG Há»¢P 2: Tham sá»‘ Ä‘Ã³ng bÄƒng (Base Model) -> Báº¯t buá»™c Float16 Ä‘á»ƒ trÃ¡nh lá»—i T4 khÃ´ng há»— trá»£ BFloat16\n",
    "    elif param.dtype == torch.bfloat16:\n",
    "        param.data = param.data.to(torch.float16)\n",
    "        print(f\"Downcast frozen param {name} to float16\")\n",
    "\n",
    "# Xá»­ lÃ½ buffers (thÆ°á»ng lÃ  mask hoáº·c position ids)\n",
    "for name, buffer in trainer.model.named_buffers():\n",
    "    if buffer.dtype == torch.bfloat16:\n",
    "        buffer.data = buffer.data.to(torch.float16)\n",
    "\n",
    "print(\"Hybrid Cast complete. LoRA is FP32, Base is FP16/INT4.\")\n",
    "\n",
    "# --- 6. Báº¯t Ä‘áº§u train ---\n",
    "trainer.train()\n",
    "\n",
    "# --- 7. LÆ°u model ---\n",
    "trainer.model.save_pretrained(NEW_MODEL_NAME)\n",
    "tokenizer.save_pretrained(NEW_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T04:10:34.959169Z",
     "iopub.status.busy": "2025-12-15T04:10:34.958876Z",
     "iopub.status.idle": "2025-12-15T04:10:43.456802Z",
     "shell.execute_reply": "2025-12-15T04:10:43.455554Z",
     "shell.execute_reply.started": "2025-12-15T04:10:34.959147Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÄÃ£ giáº£i phÃ³ng VRAM. Báº¯t Ä‘áº§u reload model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "825b0939629b406ca9273b66c439b5c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Can't find 'adapter_config.json' at 'Qwen-ViMedNLI-Hallucination-Detection'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/Qwen-ViMedNLI-Hallucination-Detection/resolve/main/adapter_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/config.py\u001b[0m in \u001b[0;36m_get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                 config_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    318\u001b[0m                     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1008\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1654\u001b[0m         \u001b[0;31m# Unauthorized => likely a token issue => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1655\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1656\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1542\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1543\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1544\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1460\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1461\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_backoff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    451\u001b[0m             )\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-693f8a43-495dc2226df65e46665ac042;efb5f233-662b-459c-a5dc-93119bd8fd88)\n\nRepository Not Found for url: https://huggingface.co/Qwen-ViMedNLI-Hallucination-Detection/resolve/main/adapter_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\nInvalid username or password.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47/652300985.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# 4. Gáº¯n Adapter (LoRA) vá»«a train vÃ o Base Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNEW_MODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Chuyá»ƒn sang cháº¿ Ä‘á»™ Ä‘Ã¡nh giÃ¡\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, key_mapping, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muse_auth_token\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"use_auth_token\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                 \u001b[0mhf_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_auth_token\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m             config = PEFT_TYPE_TO_CONFIG_MAPPING[PeftConfig._get_peft_type(model_id, **hf_kwargs)].from_pretrained(\n\u001b[0m\u001b[1;32m    460\u001b[0m                 \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/config.py\u001b[0m in \u001b[0;36m_get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m                 )\n\u001b[1;32m    322\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Can't find '{CONFIG_NAME}' at '{model_id}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mloaded_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't find 'adapter_config.json' at 'Qwen-ViMedNLI-Hallucination-Detection'"
     ]
    }
   ],
   "source": [
    "# --- 1. Dá»n dáº¹p bá»™ nhá»› VRAM (Quan trá»ng) ---\n",
    "try:\n",
    "    del model\n",
    "    del trainer\n",
    "except:\n",
    "    pass\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"ÄÃ£ giáº£i phÃ³ng VRAM. Báº¯t Ä‘áº§u reload model...\")\n",
    "\n",
    "# --- 2. Cáº¥u hÃ¬nh Ä‘Æ°á»ng dáº«n (Sá»¬A Lá»–I Táº I ÄÃ‚Y) ---\n",
    "# Äá»‹nh nghÄ©a láº¡i tÃªn model\n",
    "MODEL_NAME = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "NEW_MODEL_NAME = \"Qwen-ViMedNLI-Hallucination-Detection\"\n",
    "\n",
    "# QUAN TRá»ŒNG: ThÃªm \"./\" Ä‘á»ƒ chá»‰ Ä‘á»‹nh Ä‘Ã¢y lÃ  thÆ° má»¥c local, khÃ´ng pháº£i HuggingFace Repo\n",
    "ADAPTER_PATH = f\"./{NEW_MODEL_NAME}\"\n",
    "\n",
    "# Kiá»ƒm tra xem folder cÃ³ tá»“n táº¡i khÃ´ng Ä‘á»ƒ debug\n",
    "if os.path.exists(ADAPTER_PATH):\n",
    "    print(f\"âœ… ÄÃ£ tÃ¬m tháº¥y adapter táº¡i local: {ADAPTER_PATH}\")\n",
    "else:\n",
    "    print(f\"âš ï¸ Cáº¢NH BÃO: KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c '{ADAPTER_PATH}'. HÃ£y kiá»ƒm tra láº¡i cell training Ä‘Ã£ cháº¡y xong chÆ°a.\")\n",
    "\n",
    "# --- 3. Load láº¡i Base Model (4-bit) ---\n",
    "# Pháº£i khai bÃ¡o láº¡i bnb_config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# --- 4. Gáº¯n Adapter (LoRA) tá»« Local ---\n",
    "# Sá»­ dá»¥ng ADAPTER_PATH thay vÃ¬ NEW_MODEL_NAME\n",
    "try:\n",
    "    model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
    "    model.eval() # Chuyá»ƒn sang cháº¿ Ä‘á»™ Ä‘Ã¡nh giÃ¡\n",
    "    print(\"Load Adapter thÃ nh cÃ´ng!\")\n",
    "except Exception as e:\n",
    "    print(f\"Lá»—i khi load adapter: {e}\")\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --- 5. HÃ m dá»± Ä‘oÃ¡n ---\n",
    "def predict(messages, model, tokenizer):\n",
    "    user_prompt = messages[0]['content'] \n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": user_prompt}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=10, \n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "    generated_ids = outputs[0][inputs.input_ids.shape[1]:]\n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "# --- 6. Cháº¡y Evaluation ---\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "print(\"Starting evaluation on Test set...\")\n",
    "# Äáº£m báº£o biáº¿n dataset váº«n cÃ²n tá»“n táº¡i tá»« cÃ¡c cell trÆ°á»›c\n",
    "test_subset = dataset['test'] \n",
    "\n",
    "for i in tqdm(range(len(test_subset))):\n",
    "    sample = test_subset[i]\n",
    "    ground_truth = sample['label_str']\n",
    "    messages = sample['messages']\n",
    "    \n",
    "    try:\n",
    "        prediction = predict(messages, model, tokenizer)\n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {i}: {e}\")\n",
    "        prediction = \"Error\"\n",
    "    \n",
    "    y_true.append(ground_truth)\n",
    "    y_pred.append(prediction)\n",
    "\n",
    "# --- 7. ÄÃ¡nh giÃ¡ vÃ  lÆ°u káº¿t quáº£ (Dáº¡ng Pháº§n TrÄƒm) ---\n",
    "def clean_pred(pred):\n",
    "    pred = str(pred).lower().strip()\n",
    "    if \"Entailment\" in pred: return \"Entailment\"\n",
    "    if \"Intrinsic-Hal\" in pred: return \"Intrinsic-Hal\"\n",
    "    if \"Extrinsic-Hal\" in pred: return \"Extrinsic-Hal\"\n",
    "    return \"unknown\"\n",
    "\n",
    "y_pred_clean = [clean_pred(p) for p in y_pred]\n",
    "\n",
    "# Äá»‹nh nghÄ©a cÃ¡c nhÃ£n cáº§n Ä‘Ã¡nh giÃ¡\n",
    "target_names = [\"Entailment\", \"Intrinsic-Hal\", \"Extrinsic-Hal\"]\n",
    "\n",
    "# 1. Láº¥y bÃ¡o cÃ¡o dÆ°á»›i dáº¡ng Dictionary\n",
    "report_dict = classification_report(y_true, y_pred_clean, labels=target_names, output_dict=True)\n",
    "\n",
    "# 2. Chuyá»ƒn sang DataFrame Ä‘á»ƒ xá»­ lÃ½\n",
    "df_report = pd.DataFrame(report_dict).transpose()\n",
    "\n",
    "# 3. NhÃ¢n 100 cho cÃ¡c cá»™t Precision, Recall, F1-Score Ä‘á»ƒ ra pháº§n trÄƒm\n",
    "cols_to_scale = ['precision', 'recall', 'f1-score']\n",
    "df_report[cols_to_scale] = df_report[cols_to_scale] * 100\n",
    "\n",
    "# 4. Äá»‹nh dáº¡ng cá»™t 'support' vá» dáº¡ng sá»‘ nguyÃªn (trÃ¡nh bá»‹ hiá»ƒn thá»‹ 474.000)\n",
    "df_report['support'] = df_report['support'].astype(int)\n",
    "\n",
    "# 5. In ra mÃ n hÃ¬nh vá»›i Ä‘á»‹nh dáº¡ng 3 chá»¯ sá»‘ sau dáº¥u pháº©y\n",
    "print(\"\\nClassification Report (Unit: %):\")\n",
    "print(\"-\" * 60)\n",
    "# float_format=\"{:.3f}\".format giÃºp lÃ m trÃ²n 3 chá»¯ sá»‘\n",
    "print(df_report.to_string(float_format=\"{:.3f}\".format))\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# 6. LÆ°u káº¿t quáº£ chi tiáº¿t ra CSV\n",
    "df_result = pd.DataFrame({\n",
    "    \"Ground_Truth\": y_true, \n",
    "    \"Prediction_Raw\": y_pred, \n",
    "    \"Prediction_Clean\": y_pred_clean\n",
    "})\n",
    "df_result.to_csv(\"submission_results.csv\", index=False)\n",
    "print(\"Results saved to submission_results.csv\")\n",
    "\n",
    "# 7. (TÃ¹y chá»n) LÆ°u báº£ng Ä‘iá»ƒm ra CSV riÃªng\n",
    "df_report.to_csv(\"classification_report_percentage.csv\")\n",
    "print(\"Report saved to classification_report_percentage.csv\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8887879,
     "sourceId": 13945087,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

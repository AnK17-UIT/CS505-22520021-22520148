{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13945087,"sourceType":"datasetVersion","datasetId":8887879},{"sourceId":14377531,"sourceType":"datasetVersion","datasetId":9181799},{"sourceId":14487087,"sourceType":"datasetVersion","datasetId":9253078}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: C√†i ƒë·∫∑t ƒë·ªìng b·ªô\n!pip install -q -U torch torchvision torchaudio transformers peft datasets bitsandbytes trl accelerate pandas scikit-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T04:05:01.446433Z","iopub.execute_input":"2025-12-15T04:05:01.446644Z","iopub.status.idle":"2025-12-15T04:09:05.0017Z","shell.execute_reply.started":"2025-12-15T04:05:01.446625Z","shell.execute_reply":"2025-12-15T04:09:05.000656Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Local Inference on GPU \nModel page: https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507\n\n‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507)\n\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom datasets import Dataset, DatasetDict\n\n# Import Transformers\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM, \n    BitsAndBytesConfig, \n    TrainingArguments\n)\n\n# Import PEFT & TRL\nfrom peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\nfrom trl import SFTTrainer\nfrom sklearn.metrics import accuracy_score, classification_report\n\ntry:\n    del model\n    del trainer\n    del tokenizer\nexcept:\n    pass\ngc.collect()\ntorch.cuda.empty_cache()\n\n# Environment config\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T04:09:05.003826Z","iopub.execute_input":"2025-12-15T04:09:05.004098Z","iopub.status.idle":"2025-12-15T04:09:36.142274Z","shell.execute_reply.started":"2025-12-15T04:09:05.004069Z","shell.execute_reply":"2025-12-15T04:09:36.141588Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load model\nMODEL_NAME = \"Qwen/Qwen3-4B-Instruct-2507\"\nNEW_MODEL_NAME = \"Qwen-ViANLI-Hallucination-Detection\"\n\n# Config data path\nDATA_PATH = \"/kaggle/input/processed/processed_data_3labels/ViANLI\"\nTRAIN_FILE = os.path.join(DATA_PATH, \"train.csv\")\nDEV_FILE = os.path.join(DATA_PATH, \"dev.csv\")\nTEST_FILE = os.path.join(DATA_PATH, \"test.csv\")\n\n# Thi·∫øt l·∫≠p device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# Ki·ªÉm tra xem import c√≥ th√†nh c√¥ng kh√¥ng\nprint(\"Transformers version:\", torch.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T04:09:36.143224Z","iopub.execute_input":"2025-12-15T04:09:36.143537Z","iopub.status.idle":"2025-12-15T04:09:36.149785Z","shell.execute_reply.started":"2025-12-15T04:09:36.14351Z","shell.execute_reply":"2025-12-15T04:09:36.148879Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- COPY ƒê√à V√ÄO CELL 5 ---\ndef load_and_process_data(train_path, dev_path, test_path):\n    # Read CSV\n    df_train = pd.read_csv(train_path)\n    df_dev = pd.read_csv(dev_path)\n    df_test = pd.read_csv(test_path)\n\n    # Prompt ƒë∆∞·ª£c thi·∫øt k·∫ø kh·ªõp v·ªõi 2 nh√£n c·ªßa b·∫°n\n    def format_instruction(row):\n        prompt = f\"\"\"You are a medical AI assistant. Your task is to determine the logical relationship between the following Context and Statement.\n        \n        Context: {row['sentence1']}\n        Statement: {row['sentence2']}\n\n        Based on the context, classify the statement as one of the followings:\n        - Entailment: The statement is definitely true based on the context.\n        - Extrinsic-hal: The truth of the statement cannot be determined from the context (it might be true or false, but the context doesn‚Äôt say).\n        - Intrinsic-hal: The statement is definitely false or contradicts the context.\n\n        Answer:\n        \"\"\"\n        return prompt\n\n    def process_df(df):\n        data = []\n        for _, row in df.iterrows():\n            # Ki·ªÉm tra d·ªØ li·ªáu r·ªóng\n            if pd.isna(row['sentence1']) or pd.isna(row['sentence2']) or pd.isna(row['label']):\n                continue\n            \n            user_content = format_instruction(row)\n            \n            # --- QUAN TR·ªåNG: L·∫•y tr·ª±c ti·∫øp nh√£n text t·ª´ file CSV ---\n            # X√≥a kho·∫£ng tr·∫Øng th·ª´a n·∫øu c√≥\n            assistant_content = str(row['label']).strip() \n\n            messages = [\n                {\"role\": \"user\", \"content\": user_content},\n                {\"role\": \"assistant\", \"content\": assistant_content}\n            ]\n\n            data.append({\"messages\": messages, \"label_str\": assistant_content})\n        return Dataset.from_list(data)\n\n    print(\"Processing Train set...\")\n    train_dataset = process_df(df_train)\n    print(\"Processing Dev set...\")\n    dev_dataset = process_df(df_dev)\n    print(\"Processing Test set...\")\n    test_dataset = process_df(df_test)\n\n    return DatasetDict({\"train\": train_dataset, \"dev\": dev_dataset, \"test\": test_dataset})\n\n# Load data\ndataset = load_and_process_data(TRAIN_FILE, DEV_FILE, TEST_FILE)\nprint(\"\\nSample train data:\", dataset['train'][0])\n# Ki·ªÉm tra xem label_str in ra c√≥ ƒë√∫ng l√† \"Entailment\" ho·∫∑c \"Hallucination\" kh√¥ng","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T04:09:36.150952Z","iopub.execute_input":"2025-12-15T04:09:36.151273Z","iopub.status.idle":"2025-12-15T04:09:37.179331Z","shell.execute_reply.started":"2025-12-15T04:09:36.151245Z","shell.execute_reply":"2025-12-15T04:09:37.178546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Config Quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16, \n    bnb_4bit_use_double_quant=False,\n)\n\n# Load Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# Load Model (√âp ki·ªÉu float16 ngay t·ª´ ƒë·∫ßu)\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    torch_dtype=torch.float16 \n)\n\nmodel.config.use_cache = False \nmodel.config.pretraining_tp = 1\nmodel = prepare_model_for_kbit_training(model)\n\n# LoRA Config\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=16, \n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T04:09:37.181265Z","iopub.execute_input":"2025-12-15T04:09:37.181569Z","iopub.status.idle":"2025-12-15T04:10:22.226311Z","shell.execute_reply.started":"2025-12-15T04:09:37.181549Z","shell.execute_reply":"2025-12-15T04:10:22.22523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=8,\n    optim=\"paged_adamw_32bit\",\n    save_steps=100,\n    logging_steps=25,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=True,   \n    bf16=False, \n    max_grad_norm=0.3,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"cosine\",\n    report_to=\"none\",\n    gradient_checkpointing=True \n)\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset['train'],\n    eval_dataset=dataset['dev'],\n    peft_config=peft_config,\n    args=training_args,\n    processing_class=tokenizer,\n)\n\n# --- B∆Ø·ªöC V√Å L·ªñI (HYBRID CAST) ---\nprint(\"Applying Hybrid Cast for T4 compatibility...\")\nfor name, param in trainer.model.named_parameters():\n    if param.requires_grad:\n        if param.dtype != torch.float32:\n            param.data = param.data.to(torch.float32)\n    elif param.dtype == torch.bfloat16:\n        param.data = param.data.to(torch.float16)\n\nfor name, buffer in trainer.model.named_buffers():\n    if buffer.dtype == torch.bfloat16:\n        buffer.data = buffer.data.to(torch.float16)\nprint(\"Hybrid Cast complete.\")\n\n# Train\ntrainer.train()\n\n# Save Model Local\ntrainer.model.save_pretrained(NEW_MODEL_NAME)\ntokenizer.save_pretrained(NEW_MODEL_NAME)\nprint(\"Training finished and model saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T04:10:22.227442Z","iopub.execute_input":"2025-12-15T04:10:22.228546Z","iopub.status.idle":"2025-12-15T04:10:34.90718Z","shell.execute_reply.started":"2025-12-15T04:10:22.228518Z","shell.execute_reply":"2025-12-15T04:10:34.905678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- COPY ƒê√à V√ÄO CELL 8 ---\nimport gc\nfrom sklearn.metrics import classification_report\nimport pandas as pd\n\n# 1. D·ªçn d·∫πp b·ªô nh·ªõ\ntry:\n    del model\n    del trainer\nexcept:\n    pass\ngc.collect()\ntorch.cuda.empty_cache()\n\n# 2. ƒê·ªãnh nghƒ©a ƒë∆∞·ªùng d·∫´n local\nADAPTER_PATH = f\"./{NEW_MODEL_NAME}\"\n\n# 3. Load Base Model\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=False,\n)\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    torch_dtype=torch.float16\n)\n\n# 4. Load Adapter\nprint(f\"Loading adapter from {ADAPTER_PATH}...\")\nmodel = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\nmodel.eval()\n\n# 5. Predict Function\ndef predict(messages, model, tokenizer):\n    user_prompt = messages[0]['content'] \n    text = tokenizer.apply_chat_template(\n        [{\"role\": \"user\", \"content\": user_prompt}],\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs, \n            max_new_tokens=10, \n            pad_token_id=tokenizer.eos_token_id\n        )\n    generated_ids = outputs[0][inputs.input_ids.shape[1]:]\n    return tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n\n# 6. Run Evaluation\ny_true = []\ny_pred = []\n\nprint(\"Starting evaluation on Test set...\")\ntest_subset = dataset['test'] \n\nfor i in tqdm(range(len(test_subset))):\n    sample = test_subset[i]\n    # L·∫•y nh√£n g·ªëc t·ª´ file CSV (ƒë√£ l√† Entailment/Hallucination)\n    ground_truth = sample['label_str']\n    messages = sample['messages']\n    \n    try:\n        prediction = predict(messages, model, tokenizer)\n    except Exception as e:\n        print(f\"Error: {e}\")\n        prediction = \"unknown\"\n    \n    y_true.append(ground_truth)\n    y_pred.append(prediction)\n\n# 7. Report (Chu·∫©n h√≥a ƒë·ªÉ so s√°nh)\ndef normalize_label(label):\n    # Chuy·ªÉn h·∫øt v·ªÅ ch·ªØ th∆∞·ªùng ƒë·ªÉ so s√°nh cho ch·∫Øc ch·∫Øn\n    label = str(label).lower().strip()\n    \n    if \"Entailment\" in label:\n        return \"Entailment\"\n    if \"Intrinsic-Hal\" in label:\n        return \"Intrinsic-Hal\"\n    if \"Extrinsic-Hal\" in label:\n        return \"Extrinsic-Hal\"\n    return \"unknown\"\n\n# Chu·∫©n h√≥a c·∫£ Ground Truth v√† Prediction v·ªÅ c√πng format\ny_true_clean = [normalize_label(y) for y in y_true]\ny_pred_clean = [normalize_label(p) for p in y_pred]\n\n# L·ªçc b·ªè c√°c m·∫´u l·ªói (unknown)\nvalid_indices = [i for i, x in enumerate(y_pred_clean) if x != \"unknown\"]\ny_true_final = [y_true_clean[i] for i in valid_indices]\ny_pred_final = [y_pred_clean[i] for i in valid_indices]\n\nprint(f\"\\nS·ªë m·∫´u h·ª£p l·ªá: {len(y_true_final)}/{len(y_true)}\")\n\ntarget_names = [\"Entailment\", \"Intrinsic-Hal\", \"Extrinsic-Hal\"]\n\n# T·∫°o b√°o c√°o\nreport_dict = classification_report(\n    y_true_final, \n    y_pred_final, \n    labels=target_names, \n    output_dict=True\n)\ndf_report = pd.DataFrame(report_dict).transpose()\n\n# Nh√¢n 100 ƒë·ªÉ ra ph·∫ßn trƒÉm\ncols_to_scale = ['precision', 'recall', 'f1-score']\ndf_report[cols_to_scale] = df_report[cols_to_scale] * 100\ndf_report['support'] = df_report['support'].astype(int)\n\nprint(\"\\nClassification Report (Unit: %):\")\nprint(\"-\" * 60)\nprint(df_report.to_string(float_format=\"{:.3f}\".format))\nprint(\"-\" * 60)\n\n# L∆∞u k·∫øt qu·∫£\ndf_result = pd.DataFrame({\n    \"Ground_Truth\": y_true, \n    \"Prediction_Raw\": y_pred, \n    \"Prediction_Clean\": y_pred_clean\n})\ndf_result.to_csv(\"submission_results_vianli.csv\", index=False)\nprint(\"Results saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T04:10:34.958876Z","iopub.execute_input":"2025-12-15T04:10:34.959169Z","iopub.status.idle":"2025-12-15T04:10:43.456802Z","shell.execute_reply.started":"2025-12-15T04:10:34.959147Z","shell.execute_reply":"2025-12-15T04:10:43.455554Z"}},"outputs":[],"execution_count":null}]}